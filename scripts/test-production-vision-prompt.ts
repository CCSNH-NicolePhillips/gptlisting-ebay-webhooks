// Test Vision API with LOCAL photos using PRODUCTION prompt
import fs from "fs";
import path from "path";
import { config } from "dotenv";
import OpenAI from "openai";

config();

const PHOTOS_DIR = "testDropbox/EBAY";
const TEST_FILES = ["asd32q.jpg", "azdfkuj.jpg", "rgxbbg.jpg", "dfzdvzer.jpg"];

async function analyzeWithProductionPrompt(client: OpenAI, imagePath: string) {
  const filename = path.basename(imagePath);
  const imageBuffer = fs.readFileSync(imagePath);
  const base64 = imageBuffer.toString("base64");
  
  const prompt = `You are analyzing product photos for inventory management.

Step 1 ‚Äî ROLE DETECTION (front vs back):
‚Ä¢ Faces outward: brand logo/hero image ‚Üí negative score ‚Üí 'front'
‚Ä¢ Faces inward: ingredient list, nutrition/supplement facts table, barcode, directions, "Distributed by..." ‚Üí positive score ‚Üí 'back'
‚Ä¢ Score thresholds:
  score ‚â• +0.35 ‚Üí 'back'
  score ‚â§ ‚àí0.35 ‚Üí 'front'
  |score| < 0.2 ‚Üí 'other' (low confidence)

Step 2 ‚Äî TEXT & VISUAL EVIDENCE:
‚Ä¢ Extract ALL legible text (preserve case, line breaks). Include brand if visible anywhere (front or back).
‚Ä¢ List evidenceTriggers: exact words/visual cues that affected roleScore (e.g., 'Supplement Facts' header, barcode block near bottom-right, large hero logo, 'INGREDIENTS:').

Step 3 ‚Äî PRODUCT FIELDS:
‚Ä¢ Extract: brand, product, variant/flavor, size/servings, best-fit category, categoryPath (parent > child).
‚Ä¢ Non-product images: brand='Unknown', product='Unidentified Item'.

STRICT JSON OUTPUT (one image only):
{
  "url": "${filename}",
  "hasVisibleText": true,
  "dominantColor": "...",
  "role": "front" | "back" | "side" | "other",
  "roleScore": 0.00,
  "evidenceTriggers": ["exact texts or visual cues here"],
  "textExtracted": "<ALL visible text>"
}

CRITICAL: textExtracted must contain ALL text, especially:
- For backs: FULL ingredient lists (INCI names like "Dimethicone, Vitis Vinifera...")
- For backs: FULL supplement facts panels
- For fronts: Brand names, product names, variants`;

  console.log(`\nüîç Analyzing ${filename}...`);
  
  const response = await client.chat.completions.create({
    model: "gpt-4o",
    messages: [
      {
        role: "user",
        content: [
          { type: "text", text: prompt },
          {
            type: "image_url",
            image_url: {
              url: `data:image/jpeg;base64,${base64}`
            }
          }
        ]
      }
    ],
    max_tokens: 3000,
    temperature: 0
  });
  
  const content = response.choices[0]?.message?.content || "{}";
  
  // Extract JSON
  const jsonMatch = content.match(/\{[\s\S]*\}/);
  if (!jsonMatch) {
    console.log(`   ‚ùå No JSON in response`);
    return null;
  }
  
  try {
    const result = JSON.parse(jsonMatch[0]);
    const textLen = result.textExtracted?.length || 0;
    const hasIngredients = /INGREDIENTS:/i.test(result.textExtracted || "");
    const hasSupplementFacts = /SUPPLEMENT FACTS/i.test(result.textExtracted || "");
    
    console.log(`   Role: ${result.role} (score: ${result.roleScore})`);
    console.log(`   Text length: ${textLen} chars`);
    console.log(`   Has INGREDIENTS: ${hasIngredients ? "YES ‚úÖ" : "NO ‚ùå"}`);
    console.log(`   Has SUPPLEMENT FACTS: ${hasSupplementFacts ? "YES ‚úÖ" : "NO ‚ùå"}`);
    console.log(`   Evidence triggers: ${(result.evidenceTriggers || []).slice(0, 3).join(", ")}`);
    
    return result;
  } catch (err) {
    console.log(`   ‚ùå Failed to parse JSON: ${err}`);
    console.log(`   Response: ${content.slice(0, 300)}...`);
    return null;
  }
}

async function main() {
  console.log("üß™ TESTING VISION API WITH PRODUCTION PROMPT\n");
  console.log("Using LOCAL photos from testDropbox/EBAY\n");
  console.log("=".repeat(80) + "\n");
  
  const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
  const results: Record<string, any> = {};
  
  for (const filename of TEST_FILES) {
    const imagePath = path.join(PHOTOS_DIR, filename);
    
    if (!fs.existsSync(imagePath)) {
      console.log(`\n‚ö†Ô∏è  ${filename} not found, skipping...`);
      continue;
    }
    
    const result = await analyzeWithProductionPrompt(client, imagePath);
    results[filename] = result;
    
    // Delay between API calls
    await new Promise(resolve => setTimeout(resolve, 1500));
  }
  
  console.log("\n" + "=".repeat(80));
  console.log("\nüìä DIAGNOSIS:\n");
  
  const asd32q = results["asd32q.jpg"];
  const azdfkuj = results["azdfkuj.jpg"];
  const rgxbbg = results["rgxbbg.jpg"];
  const dfzdvzer = results["dfzdvzer.jpg"];
  
  console.log("üß¥ R+Co Product:");
  console.log(`  Front (asd32q.jpg): Text length = ${asd32q?.textExtracted?.length || 0}`);
  console.log(`  Back (azdfkuj.jpg):  Text length = ${azdfkuj?.textExtracted?.length || 0}`);
  
  const hasRCoInci = azdfkuj?.textExtracted?.includes("INGREDIENTS:") || azdfkuj?.textExtracted?.includes("Ingredients:");
  console.log(`  ‚Üí Can pair via INCI? ${hasRCoInci ? "YES ‚úÖ" : "NO ‚ùå (missing ingredient list)"}`);
  
  console.log("\nüíä Nusava Product:");
  console.log(`  Front (rgxbbg.jpg):    Text length = ${rgxbbg?.textExtracted?.length || 0}`);
  console.log(`  Back (dfzdvzer.jpg):   Text length = ${dfzdvzer?.textExtracted?.length || 0}`);
  
  const hasNusavaFacts = dfzdvzer?.textExtracted?.includes("SUPPLEMENT FACTS") || dfzdvzer?.textExtracted?.includes("Supplement Facts");
  console.log(`  ‚Üí Can pair via supplement facts? ${hasNusavaFacts ? "YES ‚úÖ" : "NO ‚ùå (missing supplement panel)"}`);
  
  console.log("\nüîç ROOT CAUSE:\n");
  
  if (!hasRCoInci) {
    console.log("‚ùå azdfkuj.jpg is missing INGREDIENTS in textExtracted");
    console.log("   ‚Üí The AUTOPAIR[hair] logic in runPairing() requires INCI detection");
    console.log("   ‚Üí This is why R+Co doesn't pair");
    if (azdfkuj?.textExtracted) {
      console.log(`\n   Preview of extracted text:`);
      console.log(`   "${azdfkuj.textExtracted.slice(0, 200)}..."`);
    }
  }
  
  if (!hasNusavaFacts) {
    console.log("‚ùå dfzdvzer.jpg is missing SUPPLEMENT FACTS in textExtracted");
    console.log("   ‚Üí The pairing logic requires supplement facts panel for backs");
    console.log("   ‚Üí This is why Nusava doesn't pair");
    if (dfzdvzer?.textExtracted) {
      console.log(`\n   Preview of extracted text:`);
      console.log(`   "${dfzdvzer.textExtracted.slice(0, 200)}..."`);
    }
  }
  
  fs.writeFileSync("vision-test-results.json", JSON.stringify(results, null, 2));
  console.log("\nüíæ Full results saved to vision-test-results.json\n");
  
  if (hasRCoInci && hasNusavaFacts) {
    console.log("‚úÖ Vision API is extracting text correctly - pairing should work!\n");
  } else {
    console.log("‚ùå Vision API is NOT extracting enough text - this is the root problem!\n");
    console.log("Solutions:");
    console.log("  1. Increase max_tokens in Vision API call (currently 3000)");
    console.log("  2. Use a different Vision model (try gpt-4o-mini or claude-3-5-sonnet)");
    console.log("  3. Split the prompt to focus only on text extraction\n");
  }
}

main().catch(err => {
  console.error("\n‚ùå ERROR:", err.message);
  console.error(err.stack);
  process.exit(1);
});
